# GNNå­¦ä¹ ç¬”è®°
> æœ¬æ–‡ç”¨æ¥è®°å½•ä¸€äº›ç›®å‰æœé›†å’Œæƒ³åˆ°çš„GNN å›¾ç¥ç»ç½‘ç»œçš„ä¸€äº›ideaå’ŒçŸ¥è¯†, æ¯”è¾ƒå…¨é¢çš„æŒ‡å¯¼ä¹¦ç±æ˜¯[William L. Hamilton](https://cs.mcgill.ca/~wlh)çš„[Graph Representation Learning](https://www.cs.mcgill.ca/~wlh/grl_book/)
>
> #### ä¸€äº›èµ„æº
>
> - [GNNæ¨èé˜…è¯»è®ºæ–‡åˆ†ç±»æ±‡æ€»Github](https://github.com/thunlp/GNNPapers)
>
> #### ä¸€äº›æ–‡ç« 
>
> - [å¦‚ä½•ç†è§£GCN](https://www.zhihu.com/question/54504471/answer/332657604) æ³¨ï¼š[å¦ä¸€ä¸ªé«˜èµå›ç­”](https://www.zhihu.com/question/54504471/answer/630639025)å¯¹æ‹‰æ™®æ‹‰æ–¯ç®—å­å’Œå›¾å·ç§¯çš„è”ç³»åšäº†å¾ˆç”ŸåŠ¨ç®€æ´çš„è¯´æ˜
> - 

## Spectral Based ConvGNN

> å­¦ä¹ è¿™ä¸ªéƒ¨åˆ†éœ€è¦å¯¹Linear Algebra æœ‰ä¸€å®šçš„æŒæ¡ï¼ŒåŒ…æ‹¬[çŸ©é˜µç‰¹å¾å€¼](https://www.zhihu.com/question/21874816/answer/181864044)ç­‰

åŸºäº**Spectral**çš„å›¾å·ç§¯ç¥ç»ç½‘ç»œï¼Œä¸»è¦æ€æƒ³æ˜¯åˆ©ç”¨ä¿¡å·å˜æ¢ç†è®ºï¼Œåˆ©ç”¨**æ‹‰æ™®æ‹‰æ–¯çŸ©é˜µï¼ˆLaplacian Matrixï¼‰**è¿›è¡Œå›¾çš„å·ç§¯å˜æ¢ï¼Œæ‹‰æ™®æ‹‰æ–¯çŸ©é˜µä¸»è¦æœ‰ä»¥ä¸‹ä¸‰ç§å®šä¹‰. *(è¿™é‡Œå‚è€ƒHamiltonä¹¦ä¸­å®šä¹‰)*ï¼š

$D$æ˜¯å›¾ä¸­å„ä¸ªå®šç‚¹çš„åº¦çš„çŸ©é˜µï¼Œ$A$æ˜¯é‚»æ¥çŸ©é˜µï¼Œå¯¹æ— å‘å›¾æ¥è¯´æ˜¯å¯¹ç§°çŸ©é˜µ

1. **$L = D - A$**  Unnormalized Laplacian 
2. **$L^{sys} = D^{-\frac{1}{2}}LD^{-\frac{1}{2}}$**   Symmetric normalized Laplacian
3. **$L^{rw} = D^{-1}L$**   Random walk normalized Laplacian



## Dynamic GNN

> the graphs are dinamic which keeps changing every seconds with edges or nodes created or deleted.(e.t. twitter Graph) -> graphs could be considered as a stream of pairwise events



## ideaå°é›†

1. å¹¿ä¹‰ä¸Šæ¥è®²ä»»ä½•æ•°æ®åœ¨èµ‹èŒƒç©ºé—´å†…éƒ½å¯ä»¥å»ºç«‹æ‹“æ‰‘å…³è”ï¼Œ[è°±èšç±»](https://www.cnblogs.com/pinard/p/6221564.html)å°±æ˜¯åº”ç”¨äº†è¿™æ ·çš„æ€æƒ³ï¼Œæ‰€ä»¥è¯´æ‹“æ‰‘è¿æ¥æ˜¯ä¸€ç§å¹¿ä¹‰çš„æ•°æ®ç»“æ„ï¼ŒGCNæœ‰å¾ˆå¤§çš„åº”ç”¨ç©ºé—´ã€‚
2. <u>Spectral Domain</u>çš„GNNå¸Œæœ›å€ŸåŠ©[å›¾è°±çš„ç†è®º](https://en.wikipedia.org/wiki/Spectral_graph_theory)æ¥å®ç°å›¾æ•°æ®ä¸Šçš„å·ç§¯æ“ä½œï¼Œä¹Ÿå°±æ˜¯å€ŸåŠ©**å›¾çš„æ‹‰æ™®æ‹‰æ–¯çŸ©é˜µçš„ç‰¹å¾å€¼å’Œç‰¹å¾å‘é‡æ¥ç ”ç©¶å›¾çš„æ€§è´¨**



## è®ºæ–‡é˜…è¯»

> è¿™ä¸ªéƒ¨åˆ†æ˜¯ä¸€äº›é˜…è¯»çš„ç»å…¸è®ºæ–‡çš„ä¸»ä½“æ€è·¯å’Œä»‹ç»



### GraphSAGE

GraphSAGEæ˜¯ä¸€ç¯‡ç»å…¸çš„æ¢ç©¶graph scalabilityæ¥åšbatch trainningçš„è®ºæ–‡

#### Main Idea

The main idea is that in order to compute the training loss on a single node with an *L*-layer GCN, only the *L*-hop neighbours of that node are necessary, as nodes further away in the graph are not involved in the computation. 

The problem is that, for graphs of the â€œ[small-world](https://en.wikipedia.org/wiki/Small-world_network#:~:text=A small-world network is,number of hops or steps.)â€ type, such as social networks, the 2-hop neighbourhood of some nodes may already contain millions of nodes, making it too big to be stored in memory [2]. GraphSAGE <u>tackles this problem</u> by sampling the neighbours up to the *L*-th hop: starting from the training node, it samples uniformly with replacement[1] a fixed number *k* of 1-hop neighbours, then for each of these neighbours it again samples *k* neighbours, and so on for *L* times. In this way, for every node we are guaranteed to have a bounded *L*-hop sampled neighbourhood of ğ’ª(*ká´¸*) nodes. If we then construct a batch with *b* training nodes, each with its own independent *L*-hop neighbourhood, we get to a memory complexity of ğ’ª(*bká´¸*) independent of the graph size *n*. The computational complexity of one batch of GraphSAGE is ğ’ª(*bLd*Â²*ká´¸*).

> [1]Sampling with replacement means that some neighbour nodes can appear more than once, **in particular if the number of neighbours is smaller than *k*.**
>
> [2]The number of neighbours in such graphs tends to grow exponentially with the neighbourhood expansion.

#### Drawbacks of GraphSAGE

* A notable drawback of GraphSAGE is that sampled nodes might appear multiple times, thus potentially introducing a lot of redundant computation.

